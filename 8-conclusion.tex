In this paper, we observe that the existing resource orchestrators like \textit{Kubernetes} overlook the importance of dynamic orchestration of GPUs. Hence, we identify several challenges in dynamic resource orchestration for GPU-based datacenters.  We explore the possibilities of QoS-aware power proportional scheduling that arise by exposing the GPU APIs to the system orchestrator. Motivated by our observations, we propose \textit{Kube-Knots} by integrating \textit{Knots}, a GPU aware orchestration layer with \textit{Kubernetes} for GPU utilization aware orchestration. We further evaluate three GPU-based scheduling techniques on a ten node GPU-cluster, which leverages \textit{Kube-Knots} for GPU container orchestration.

Our proposed Peak Prediction (PP) scheduler when compared against the \textit{Kubernetes} uniform scheduler, improve the cluster-wide GPU utilization by up to 80\% for both 50$^{th}$ and 90$^{th}$ percentile, through QoS aware workload consolidation which leads to 33\% savings in power consumption across the cluster. Further, we reduced the overall QoS violations of latency sensitive queries by up to 53\% when compared to the resource agnostic scheduler with GPU utilization prediction accuracy as high as at 84\%. We further plan to extend the \textit{Knots} orchestration framework for other accelerators like FPGAs and TPUs.


%However, correlation between co-scheduled GPU applications can lead to significant capacity violations if consolidation methodologies do not take them into account. We design two new placement methodologies which ensures consolidation through CBP and PCP that use an off-peak metric for sizing and another metric to ensure that peaks do not lead to violations. Our experimental evaluation shows that Peak prediction achieves superior power savings, low violations and good load balance across active GPUs in a datacenter.